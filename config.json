{
  // Which model to auto-load
  "default_model": "llama_3.1_8b_8q",

  "models": [
    {
      "key": "llama_3.2_3b_6q",
      "url": "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K_L.gguf",
      "path": "models/Llama-3.2-3B-Instruct-Q6_K_L.gguf",

      "llama_kw": {
        // ----------------------------
        // ðŸ”§ Useful to Change
        // ----------------------------
        "n_ctx": 8192,           // context window
        "n_batch": 512,          // batch throughput
        "n_threads": 12,         // CPU threads
        "n_gpu_layers": 0,       // 0 = CPU, >0 = GPU offload
        "use_mmap": true,        // faster loading
        "use_mlock": true,       // lock in RAM (set false if permission error)
        "f16_kv": true,          // reduce KV memory, small perf cost
        "verbose": false,        // enable llama.cpp debug logs

        // ----------------------------
        // ðŸ§  Advanced / Niche Settings
        // ----------------------------
        "rope_scaling_type": null,
        "split_mode": null,
        "main_gpu": null,
        "tensor_split": null,
        "vocab_only": null,
        "kv_overrides": null,
        "seed": null,
        "n_ubatch": null,
        "n_threads_batch": null,
        "pooling_type": null,
        "rope_freq_base": null,
        "rope_freq_scale": null,
        "yarn_ext_factor": null,
        "yarn_attn_factor": null,
        "yarn_beta_fast": null,
        "yarn_beta_slow": null,
        "yarn_orig_ctx": null,
        "logits_all": null,
        "embedding": null,
        "offload_kqv": null,
        "flash_attn": null,
        "op_offload": null,
        "swa_full": null,
        "no_perf": null,
        "last_n_tokens_size": null,
        "lora_base": null,
        "lora_scale": null,
        "lora_path": null,
        "numa": null,
        "chat_format": null,
        "chat_handler": null,
        "draft_model": null,
        "tokenizer": null,
        "type_k": null,
        "type_v": null,
        "spm_infill": null
      }
    },

    {
      "key": "llama_3.1_8b_8q",
      "url": "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
      "path": "models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",

      "llama_kw": {
        // ----------------------------
        // ðŸ”§ Useful to Change
        // ----------------------------
        "n_ctx": 16384,          // context window
        "n_batch": 512,          // batch throughput
        "n_threads": 12,         // CPU threads
        "n_gpu_layers": 0,       // 0 = CPU, >0 = GPU offload
        "use_mmap": true,        // faster loading
        "use_mlock": true,       // lock in RAM (set false if permission error)
        "f16_kv": true,          // reduce KV memory, small perf cost
        "verbose": false,        // enable llama.cpp debug logs

        // ----------------------------
        // ðŸ§  Advanced / Niche Settings
        // ----------------------------
        "rope_scaling_type": null,
        "split_mode": null,
        "main_gpu": null,
        "tensor_split": null,
        "vocab_only": null,
        "kv_overrides": null,
        "seed": null,
        "n_ubatch": null,
        "n_threads_batch": null,
        "pooling_type": null,
        "rope_freq_base": null,
        "rope_freq_scale": null,
        "yarn_ext_factor": null,
        "yarn_attn_factor": null,
        "yarn_beta_fast": null,
        "yarn_beta_slow": null,
        "yarn_orig_ctx": null,
        "logits_all": null,
        "embedding": null,
        "offload_kqv": null,
        "flash_attn": null,
        "op_offload": null,
        "swa_full": null,
        "no_perf": null,
        "last_n_tokens_size": null,
        "lora_base": null,
        "lora_scale": null,
        "lora_path": null,
        "numa": null,
        "chat_format": null,
        "chat_handler": null,
        "draft_model": null,
        "tokenizer": null,
        "type_k": null,
        "type_v": null,
        "spm_infill": null
      }
    }
  ]
}
